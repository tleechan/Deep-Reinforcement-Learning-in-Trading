{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Instructions\n",
    "\n",
    "1. If you are new to Jupyter notebooks, please go through this introductory manual <a href='https://quantra.quantinsti.com/quantra-notebook' target=\"_blank\">here</a>.\n",
    "1. Any changes made in this notebook would be lost after you close the browser window. **You can download the notebook to save your work on your PC.**\n",
    "1. Before running this notebook on your local PC:<br>\n",
    "i.  You need to set up a Python environment and the relevant packages on your local PC. To do so, go through the section on \"**Run Codes Locally on Your Machine**\" in the course.<br>\n",
    "ii. You need to **download the zip file available in the last unit** of this course. The zip file contains the data files and/or python modules that might be required to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project Template: Building the RL model\n",
    "\n",
    "In this notebook, we provide a template to build a reinforcement learning model for the capstone project. You can build your solution here. The features and parameters suggested are not exhaustive or fine-tuned. The RL model needs to be calibrated separately for each underlying asset it trades on.\n",
    "\n",
    "The template gives you a structure to answer the problem statements posed in the capstone project. The notebook structure is as follows:\n",
    "\n",
    "1. [Read price data](#Read_price_data): Get any stock data, FX data or any other asset data of your choice. This data needs to be of minute frequency. We have provided a sample FX currency pair data in the model solution. \n",
    "<br>\n",
    "\n",
    "1. [Data sanity check](#sanity): Perform data sanity checks such as the check for missing values and the check for outliers. Then, you can either fix the data or get good quality data.\n",
    "<br>\n",
    "1. [Game class and input features](#input_features): We make use of TA-lib for creating technical features. For fundamental features, we need to obtain data from external sources. Example features added here:\n",
    "    * Statistical features: Beta of high and low values\n",
    "    * Overlap Studies: Change simple moving average to exponential moving average\n",
    "    * Volatility indicator: ATR (Average True Range)\n",
    "    * Other asset price data: On balance volume\n",
    "<br>\n",
    "<br>\n",
    "1. [Reward function](#reward): Think about the outcome you are looking for from the RL model and select the reward function accordingly. You can use the existing reward functions or make your own.\n",
    "<br>\n",
    "1. [Experience replay sampling](#replay): Change the uniform randomly sampling approach to recency sampling. In the recency sampling approach, select N most recent samples from the memory buffer. This can be then restored to uniform random sampling. Recency sampling is expected to perform badly but will act as the baseline to assess the impact of experience replay on the RL agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Import_modules'></a> \n",
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import talib\n",
    "import pickle\n",
    "import pyfolio as pf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Read_price_data'></a> \n",
    "## Read price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the price data\n",
    "bars5m = pd.read_pickle('../data_modules/fx_pair_data_1.bz2')\n",
    "\n",
    "# Display the last 5 entries of price data\n",
    "bars5m.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sanity'></a> \n",
    "## Data sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Add code for checking missing data.\n",
    "You can drop the incomplete days of data, or obtain good quality data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Add code for invalid entries in data (NaN values).\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='input_features'></a> \n",
    "## Game class and input features\n",
    "We define the game class for the RL agent. The `_assemble_state` function can be edited to add any number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game(object):\n",
    "\n",
    "    def __init__(self, bars5m, bars1d, bars1h, reward_function, lkbk=20, init_idx=None):\n",
    "        self.bars5m = bars5m\n",
    "        self.lkbk = lkbk\n",
    "        self.trade_len = 0\n",
    "        self.stop_pnl = None\n",
    "        self.bars1d = bars1d\n",
    "        self.bars1h = bars1h\n",
    "        self.is_over = False\n",
    "        self.reward = 0\n",
    "        self.pnl_sum = 0\n",
    "        self.init_idx = init_idx\n",
    "        self.reward_function = reward_function\n",
    "        self.reset()\n",
    "\n",
    "    def _update_position(self, action):\n",
    "        '''This is where we update our position'''\n",
    "        if action == 0:\n",
    "            pass\n",
    "\n",
    "        elif action == 2:\n",
    "            \"\"\"---Enter a long or exit a short position---\"\"\"\n",
    "\n",
    "            # If the current position (buy) is the same as the action (buy), do nothing\n",
    "            if self.position == 1:\n",
    "                pass\n",
    "\n",
    "            # If there is no current position, we update the position to indicate buy\n",
    "            elif self.position == 0:\n",
    "                self.position = 1\n",
    "                self.entry = self.curr_price\n",
    "                self.start_idx = self.curr_idx\n",
    "\n",
    "            # If action is different than current position, we end the game and get rewards & trade duration\n",
    "            elif self.position == -1:\n",
    "                self.is_over = True\n",
    "\n",
    "        elif action == 1:\n",
    "            \"\"\"---Enter a short or exit a long position---\"\"\"\n",
    "            if self.position == -1:\n",
    "                pass\n",
    "\n",
    "            elif self.position == 0:\n",
    "                self.position = -1\n",
    "                self.entry = self.curr_price\n",
    "                self.start_idx = self.curr_idx\n",
    "\n",
    "            elif self.position == 1:\n",
    "                self.is_over = True\n",
    "\n",
    "    def _assemble_state(self):\n",
    "        '''Here we can add secondary features such as indicators and times to our current state.\n",
    "        First, we create candlesticks for different bar sizes of 5mins, 1hr and 1d.\n",
    "        We then add some state variables such as time of day, day of week and position.\n",
    "        Next, several indicators are added and subsequently z-scored.\n",
    "        '''\n",
    "\n",
    "        \"\"\"---Initializing State Variables---\"\"\"\n",
    "        self.state = np.array([])\n",
    "\n",
    "        self._get_last_N_timebars()\n",
    "\n",
    "        \"\"\"\"---Adding Normalised Candlesticks---\"\"\"\n",
    "\n",
    "        def _get_normalised_bars_array(bars):\n",
    "            bars = bars.iloc[-10:, :-1].values.flatten()\n",
    "            \"\"\"Normalizing candlesticks\"\"\"\n",
    "            bars = (bars-np.mean(bars))/np.std(bars)\n",
    "            return bars\n",
    "\n",
    "        self.state = np.append(\n",
    "            self.state, _get_normalised_bars_array(self.last5m))\n",
    "        self.state = np.append(\n",
    "            self.state, _get_normalised_bars_array(self.last1h))\n",
    "        self.state = np.append(\n",
    "            self.state, _get_normalised_bars_array(self.last1d))\n",
    "\n",
    "        \"\"\"---Adding Techincal Indicators---\"\"\"\n",
    "\n",
    "        def _get_technical_indicators(bars):\n",
    "            # Create an array to store the value of indicators\n",
    "            tech_ind = np.array([])\n",
    "\n",
    "            \"\"\"Relative Strength Index\"\"\"\n",
    "            tech_ind = np.append(tech_ind, talib.RSI(\n",
    "                bars['close'], self.lkbk-1)[-1])\n",
    "\n",
    "            \"\"\"Momentum\"\"\"\n",
    "            tech_ind = np.append(tech_ind, talib.MOM(\n",
    "                bars['close'], self.lkbk-1)[-1])\n",
    "\n",
    "            \"\"\"Balance of Power\"\"\"\n",
    "            tech_ind = np.append(tech_ind, talib.BOP(bars['open'],\n",
    "                                                     bars['high'],\n",
    "                                                     bars['low'],\n",
    "                                                     bars['close'])[-1])\n",
    "\n",
    "            \"\"\"Aroon Oscillator\"\"\"\n",
    "            tech_ind = np.append(tech_ind, talib.AROONOSC(bars['high'],\n",
    "                                                          bars['low'],\n",
    "                                                          self.lkbk-3)[-1])\n",
    "            \n",
    "            '''\n",
    "            You can add as many input features you want in this section.\n",
    "            In addition to the above, let's add the indicators mentioned in the capstone project model solution.\n",
    "        \n",
    "            '''\n",
    "            \n",
    "            # Indicators as input features for the capstone project\n",
    "            \n",
    "            # --------------- Statistic Functions --------------------------\n",
    "            \"\"\"Beta of the high and low values\"\"\"\n",
    "             \n",
    "            \"\"\"###--------------- Add code for indicator here ---------------###\"\"\"\n",
    "            # --------------------------------------------------------------\n",
    "\n",
    "            \n",
    "            # --------------- Overlap Studies ------------------------------\n",
    "            \"\"\"Relative difference two moving averages\"\"\"\n",
    "            \"\"\"###--------------- Add code for indicator here ---------------###\"\"\"\n",
    "            # --------------------------------------------------------------  \n",
    "            \n",
    "            \n",
    "            # --------------- Volatility Indicators ------------------------\n",
    "            \n",
    "            \"\"\"Standard Deviation\"\"\"\n",
    "            \"\"\"###--------------- Add code for indicator here ---------------###\"\"\"\n",
    "            \n",
    "            \"\"\"Average True Range\"\"\"\n",
    "            \"\"\"###--------------- Add code for indicator here ---------------###\"\"\"\n",
    "            # -------------------------------------------------------------- \n",
    "            \n",
    "            \n",
    "            # --------------- Volume Indicators ------------------------\n",
    "            \"\"\"On Balance Volume\"\"\"\n",
    "            \"\"\"###--------------- Add code for indicator here ---------------###\"\"\"\n",
    "            # -------------------------------------------------------------- \n",
    "            \n",
    "            return tech_ind\n",
    "\n",
    "        self.state = np.append(\n",
    "            self.state, _get_technical_indicators(self.last5m))\n",
    "        self.state = np.append(\n",
    "            self.state, _get_technical_indicators(self.last1h))\n",
    "        self.state = np.append(\n",
    "            self.state, _get_technical_indicators(self.last1d))\n",
    "\n",
    "        \"\"\"---Adding Time Signature---\"\"\"\n",
    "        self.curr_time = self.bars5m.index[self.curr_idx]\n",
    "        tm_lst = list(map(float, str(self.curr_time.time()).split(':')[:2]))\n",
    "        self._time_of_day = (tm_lst[0]*60 + tm_lst[1])/(24*60)\n",
    "        self._day_of_week = self.curr_time.weekday()/6\n",
    "        self.state = np.append(self.state, self._time_of_day)\n",
    "        self.state = np.append(self.state, self._day_of_week)\n",
    "\n",
    "        \"\"\"---Adding Position---\"\"\"\n",
    "        self.state = np.append(self.state, self.position)\n",
    "\n",
    "    def _get_last_N_timebars(self):\n",
    "        '''This function gets the timebars for the 5m, 1hr and 1d resolution based\n",
    "        on the lookback we've specified.\n",
    "        '''\n",
    "        wdw5m = 9\n",
    "        wdw1h = np.ceil(self.lkbk*15/24.)\n",
    "        wdw1d = np.ceil(self.lkbk*15)\n",
    "\n",
    "        \"\"\"---Getting candlesticks before current time---\"\"\"\n",
    "        self.last5m = self.bars5m[self.curr_time -\n",
    "                                  timedelta(wdw5m):self.curr_time].iloc[-self.lkbk:]\n",
    "        self.last1h = self.bars1h[self.curr_time -\n",
    "                                  timedelta(wdw1h):self.curr_time].iloc[-self.lkbk:]\n",
    "        self.last1d = self.bars1d[self.curr_time -\n",
    "                                  timedelta(wdw1d):self.curr_time].iloc[-self.lkbk:]\n",
    "\n",
    "    def _get_reward(self):\n",
    "        \"\"\"Here we calculate the reward when the game is finished.\n",
    "        Reward function design is very difficult and can significantly\n",
    "        impact the performance of our algo.\n",
    "        In this case, we use a simple pnl reward but it is conceivable to use\n",
    "        other metrics such as Sharpe ratio, average return, etc.\n",
    "        \"\"\"\n",
    "        if self.is_over:\n",
    "            self.reward = self.reward_function(\n",
    "                self.entry, self.curr_price, self.position)\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"This function returns the state of the system.\n",
    "        Returns:\n",
    "            self.state: the state including indicators, position and times.\n",
    "        \"\"\"\n",
    "        # Assemble new state\n",
    "        self._assemble_state()\n",
    "        return np.array([self.state])\n",
    "\n",
    "    def act(self, action):\n",
    "        \"\"\"This function updates the state based on an action\n",
    "        that was calculated by the NN.\n",
    "        This is the point where the game interacts with the trading\n",
    "        algo.\n",
    "        \"\"\"\n",
    "\n",
    "        self.curr_time = self.bars5m.index[self.curr_idx]\n",
    "        self.curr_price = self.bars5m['close'][self.curr_idx]\n",
    "\n",
    "        self._update_position(action)\n",
    "\n",
    "        # Unrealized or realized pnl. This is different from pnl in reward method which is only realized pnl.\n",
    "        self.pnl = (-self.entry + self.curr_price)*self.position/self.entry\n",
    "\n",
    "        self._get_reward()\n",
    "        if self.is_over:\n",
    "            self.trade_len = self.curr_idx - self.start_idx\n",
    "\n",
    "        return self.reward, self.is_over\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resetting the system for each new trading game.\n",
    "        Here, we also resample the bars for 1h and 1d.\n",
    "        Ideally, we should do this on every update but this will take very long.\n",
    "        \"\"\"\n",
    "        self.pnl = 0\n",
    "        self.entry = 0\n",
    "        self._time_of_day = 0\n",
    "        self._day_of_week = 0\n",
    "        self.curr_idx = self.init_idx\n",
    "        self.t_in_secs = (\n",
    "            self.bars5m.index[-1]-self.bars5m.index[0]).total_seconds()\n",
    "        self.start_idx = self.curr_idx\n",
    "        self.curr_time = self.bars5m.index[self.curr_idx]\n",
    "        self._get_last_N_timebars()\n",
    "        self.position = 0\n",
    "        self.act(0)\n",
    "        self.state = []\n",
    "        self._assemble_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Defining the method to create the neural networks\n",
    "'''\n",
    "def init_net(env, rl_config):\n",
    "    \"\"\"\n",
    "    This initialises the RL run by\n",
    "    creating two new predictive neural network\n",
    "    Args:\n",
    "        env:\n",
    "    Returns:\n",
    "        modelQ: the neural network\n",
    "        modelR: the neural network\n",
    "\n",
    "    \"\"\"\n",
    "    hidden_size = len(env.state)*rl_config['HIDDEN_MULT']\n",
    "    modelQ = Sequential()\n",
    "    modelQ.add(Dense(len(env.state), input_shape=(\n",
    "        len(env.state),), activation=rl_config['ACTIVATION_FUN']))\n",
    "    modelQ.add(Dense(hidden_size, activation=rl_config['ACTIVATION_FUN']))\n",
    "    modelQ.add(Dense(rl_config['NUM_ACTIONS'], activation='softmax'))\n",
    "    modelQ.compile(SGD(lr=rl_config['LEARNING_RATE']), loss=rl_config['LOSS_FUNCTION'])\n",
    "\n",
    "    modelR = Sequential()\n",
    "    modelR.add(Dense(len(env.state), input_shape=(\n",
    "        len(env.state),), activation=rl_config['ACTIVATION_FUN']))\n",
    "    modelR.add(Dense(hidden_size, activation=rl_config['ACTIVATION_FUN']))\n",
    "    modelR.add(Dense(rl_config['NUM_ACTIONS'], activation='softmax'))\n",
    "    modelR.compile(SGD(lr=rl_config['LEARNING_RATE']), loss=rl_config['LOSS_FUNCTION'])\n",
    "\n",
    "    return modelQ, modelR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='reward'></a> \n",
    "## Reward function\n",
    "\n",
    "Define the various reward functions here. You can define custom reward functions here. The particular reward function to use for the RL agent will be specified in the `rl_config` dictionary which is used as the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pnl(entry, curr, pos):\n",
    "    # Transaction cost and commissions\n",
    "    tc = 0.001\n",
    "    return (curr*(1-tc) - entry*(1+tc))/entry*(1+tc)*pos\n",
    "\n",
    "\n",
    "def reward_pos_log_pnl(entry, curr, pos):\n",
    "    \"\"\"positive log categorical\"\"\"\n",
    "    pnl = get_pnl(entry, curr, pos)\n",
    "    if pnl >= 0:\n",
    "        return np.ceil(np.log(pnl*100+1))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def reward_pure_pnl(entry, curr, pos):\n",
    "    '''pure pnl'''\n",
    "    return get_pnl(entry, curr, pos)\n",
    "\n",
    "\n",
    "def reward_positive_pnl(entry, curr, pos):\n",
    "    '''positive pnl, zero otherwise'''\n",
    "    pnl = get_pnl(entry, curr, pos)\n",
    "    if pnl >= 0:\n",
    "        return pnl\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def reward_categorical_pnl(entry, curr, pos):\n",
    "    '''Sign of pnl'''\n",
    "    return np.sign(get_pnl(entry, curr, pos))\n",
    "\n",
    "\n",
    "def reward_positive_categorical_pnl(entry, curr, pos):\n",
    "    '''1 for win, 0 for loss'''\n",
    "    pnl = get_pnl(entry, curr, pos)\n",
    "    if pnl >= 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def reward_exponential_pnl(entry, curr, pos):\n",
    "    '''exp pure pnl'''\n",
    "    return np.exp(get_pnl(entry, curr, pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='replay'></a> \n",
    "## Experience replay\n",
    "\n",
    "We define the experience replay class here. The `process` function is used to implement the experience replay.\n",
    "The random sampling as well as recency sampling is provided in code and you may use these as you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay(object):\n",
    "    '''This class calculates the Q-Table.\n",
    "    It gathers memory from previous experience and \n",
    "    creates a Q-Table with states and rewards for each\n",
    "    action using the NN. At the end of the game the reward\n",
    "    is calculated from the reward function. \n",
    "    The weights in the NN are constantly updated with new\n",
    "    batch of experience. \n",
    "    This is the heart of the RL algorithm.\n",
    "    Args:\n",
    "        state_tp1: the state at time t+1\n",
    "        state_t: the state at time t\n",
    "        action_t: int {0..2} hold, sell, buy taken at state_t \n",
    "        Q_sa: float, the reward for state_tp1\n",
    "        reward_t: the reward for state_t\n",
    "        self.memory: list of state_t, action_t and reward_t at time t as well as state_tp1\n",
    "        targets: array(float) Nx2, weight of each action\n",
    "        inputs: an array with scrambled states at different times\n",
    "        targets: Nx3 array of weights for each action for scrambled input states\n",
    "    '''\n",
    "\n",
    "    def __init__(self, max_memory, discount):\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "\n",
    "    def remember(self, states, game_over):\n",
    "        '''Add states to time t and t+1 as well as  to memory'''\n",
    "        self.memory.append([states, game_over])\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def process(self, modelQ, modelR, batch_size=10):\n",
    "        len_memory = len(self.memory)\n",
    "        num_actions = modelQ.output_shape[-1]\n",
    "        env_dim = self.memory[0][0][0].shape[1]\n",
    "\n",
    "        \"\"\"---Initialise input and target arrays---\"\"\"\n",
    "        inputs = np.zeros((min(len_memory, batch_size), env_dim))\n",
    "        targets = np.zeros((inputs.shape[0], num_actions))\n",
    "        \n",
    "    \n",
    "                \n",
    "        # for i, idx in enumerate(np.arange(-inputs.shape[0],0)): ### This is the random sampling ###\n",
    "        for \"\"\"###--------------- Add the for loop for recency sampling here ---------------###\"\"\"  \n",
    "            \"\"\"Obtain the parameters for Bellman from memory,\n",
    "            S.A.R.S: state, action, reward, new state.\"\"\"\n",
    "            state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
    "            game_over = self.memory[idx][1]\n",
    "            inputs[i] = state_t\n",
    "\n",
    "            \"\"\"---Calculate the targets for the state at time t---\"\"\"\n",
    "            targets[i] = modelR.predict(state_t)[0]\n",
    "\n",
    "            \"\"\"---Calculate the reward at time t+1 for action at time t---\"\"\"\n",
    "            Q_sa = np.max(modelQ.predict(state_tp1)[0])\n",
    "\n",
    "            if game_over:\n",
    "                \"\"\"---When game is over we have a definite reward---\"\"\"\n",
    "                targets[i, action_t] = reward_t\n",
    "            else:\n",
    "                \"\"\"\n",
    "                ---Update the part of the target for which action_t occured to new value---\n",
    "                Q_new(s,a) = reward_t + gamma * max_a' Q(s', a')\n",
    "                \"\"\"\n",
    "                targets[i, action_t] = reward_t + self.discount * Q_sa\n",
    "\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='backtesting'></a> \n",
    "## Backtesting function\n",
    "\n",
    "We define the `run` function. It is used by the RL agent to train on the historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(bars5m, rl_config):\n",
    "    \"\"\"\n",
    "    Function to run the RL model on the passed price data\n",
    "    \"\"\"\n",
    "    \n",
    "    pnls = []\n",
    "    trade_logs = pd.DataFrame()\n",
    "    episode = 0\n",
    "\n",
    "    ohlcv_dict = {\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last',\n",
    "        'volume': 'sum'\n",
    "    }\n",
    "\n",
    "    bars1h = bars5m.resample('1H', label='left', closed='right').agg(ohlcv_dict).dropna()\n",
    "    bars1d = bars1h.resample('1D', label='left', closed='right').agg(ohlcv_dict).dropna()\n",
    "\n",
    "    \"\"\"---Initialise a NN and a set up initial game parameters---\"\"\"\n",
    "    env = Game(bars5m, bars1d, bars1h, rl_config['RF'],\n",
    "               lkbk=rl_config['LKBK'], init_idx=rl_config['START_IDX'])\n",
    "    q_network, r_network = init_net(env, rl_config)\n",
    "    exp_replay = ExperienceReplay(max_memory=rl_config['MAX_MEM'], discount=rl_config['DISCOUNT_RATE'])\n",
    "    \"\"\"---Preloading the model weights---\"\"\"\n",
    "    if rl_config['PRELOAD']:\n",
    "        q_network.load_weights(rl_config['WEIGHTS_FILE'])\n",
    "        r_network.load_weights(rl_config['WEIGHTS_FILE'])\n",
    "        exp_replay.memory = pickle.load(open(rl_config['REPLAY_FILE'], 'rb'))\n",
    "\n",
    "    r_network.set_weights(q_network.get_weights())\n",
    "\n",
    "    \"\"\"---Loop that steps through one trade (game) at a time---\"\"\"\n",
    "    while True:\n",
    "        \"\"\"---Stop the algo when end is near to avoid exception---\"\"\"\n",
    "        if env.curr_idx >= len(bars5m)-1:\n",
    "            break\n",
    "\n",
    "        episode += 1\n",
    "\n",
    "        \"\"\"---Initialise a new game---\"\"\"\n",
    "        env = Game(bars5m, bars1d, bars1h, rl_config['RF'],\n",
    "                   lkbk=rl_config['LKBK'], init_idx=env.curr_idx)\n",
    "        state_tp1 = env.get_state()\n",
    "\n",
    "        \"\"\"---Calculate epsilon for exploration vs exploitation random action generator---\"\"\"\n",
    "        epsilon = rl_config['EPSILON']**(np.log10(episode))+rl_config['EPS_MIN']\n",
    "\n",
    "        game_over = False\n",
    "        cnt = 0\n",
    "\n",
    "        \"\"\"---Walkthrough time steps starting from the end of the last game---\"\"\"\n",
    "        while not game_over:\n",
    "        \n",
    "            if env.curr_idx >= len(bars5m)-1:\n",
    "                break\n",
    "\n",
    "            cnt += 1\n",
    "            state_t = state_tp1\n",
    "\n",
    "            \"\"\"---Generate a random action or through q_network---\"\"\"\n",
    "            if np.random.rand() <= epsilon:\n",
    "                action = np.random.randint(0, 3, size=1)[0]\n",
    "\n",
    "            else:\n",
    "                q = q_network.predict(state_t)\n",
    "                action = np.argmax(q[0])\n",
    "\n",
    "            \"\"\"---Updating the Game---\"\"\"\n",
    "            reward, game_over = env.act(action)\n",
    "\n",
    "            \"\"\"---Updating trade/position logs---\"\"\"\n",
    "            tl = [[env.curr_time, env.position, episode]]\n",
    "            if game_over:\n",
    "                tl = [[env.curr_time, 0, episode]]\n",
    "            trade_logs = trade_logs.append(tl)\n",
    "\n",
    "            \"\"\"---Move to next time step---\"\"\"\n",
    "            env.curr_idx += 1\n",
    "            state_tp1 = env.get_state()\n",
    "\n",
    "            \"\"\"---Adding state to memory---\"\"\"\n",
    "            exp_replay.remember(\n",
    "                [state_t, action, reward, state_tp1], game_over)\n",
    "\n",
    "            \"\"\"---Creating a new Q-Table---\"\"\"\n",
    "            inputs, targets = exp_replay.process(\n",
    "                q_network, r_network, batch_size=rl_config['BATCH_SIZE'])\n",
    "            env.pnl_sum = sum(pnls)\n",
    "\n",
    "            \"\"\"---Update the NN model with a new Q-Table\"\"\"\n",
    "            q_network.train_on_batch(inputs, targets)\n",
    "\n",
    "            if game_over and rl_config['UPDATE_QR']:\n",
    "                r_network.set_weights(q_network.get_weights())\n",
    "\n",
    "        pnls.append(env.pnl)\n",
    "\n",
    "        print(\"Trade {:03d} | pos {} | len {} | approx cum ret {:,.2f}% | trade ret {:,.2f}% | eps {:,.4f} | {} | {}\".format(\n",
    "            episode, env.position, env.trade_len, sum(pnls)*100, env.pnl*100, epsilon, env.curr_time, env.curr_idx))\n",
    "\n",
    "        if not episode % 10:\n",
    "            print('----saving weights, trade logs and replay buffer-----')\n",
    "            r_network.save_weights(rl_config['WEIGHTS_FILE'], overwrite=True)\n",
    "            trade_logs.to_pickle(rl_config['TRADE_FILE'])\n",
    "            pickle.dump(exp_replay.memory, open(rl_config['REPLAY_FILE'], 'wb'))\n",
    "\n",
    "        if not episode % 15 and rl_config['TEST_MODE']:\n",
    "            print('\\n**********************************************\\nTest mode is on due to resource constraints and therefore stopped after 15 trades. \\nYou can trade on full dataset on your local computer and set TEST_MODE flag to False in rl_config dictionary. \\nThe full code file, quantra_reinforemcent_learning module and data file is available in last unit of the course.\\n**********************************************\\n')\n",
    "            break\n",
    "\n",
    "    print('----saving weights, trade logs and replay buffer-----')\n",
    "    r_network.save_weights(rl_config['WEIGHTS_FILE'], overwrite=True)\n",
    "    trade_logs.to_pickle(rl_config['TRADE_FILE'])\n",
    "    pickle.dump(exp_replay.memory, open(rl_config['REPLAY_FILE'], 'wb'))\n",
    "\n",
    "    print('***FINISHED***')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='run'></a> \n",
    "# RL configuration and running the backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For running the RL model on the price data, you need to \n",
    "set the configuration parameters.\n",
    "These configuration parameters are hyperparameters for the \n",
    "RL model and the ANN used in it.\n",
    "\"\"\"\n",
    "rl_config = {\n",
    "    \n",
    "             'LEARNING_RATE': 0.00001,\n",
    "             'LOSS_FUNCTION': 'mse',\n",
    "             'ACTIVATION_FUN': 'relu',\n",
    "             'NUM_ACTIONS': 3,\n",
    "             'HIDDEN_MULT': 2,\n",
    "             'DISCOUNT_RATE': 0.9,\n",
    "             'LKBK': 30,\n",
    "             'BATCH_SIZE': 1,\n",
    "             'MAX_MEM': 600, ### You can change the maxmimum memory buffer here ###\n",
    "             'EPSILON': 0.0001,\n",
    "             'EPS_MIN': 0.001,\n",
    "             'START_IDX': 5000,\n",
    "             'WEIGHTS_FILE': 'indicator_model_fx_pair_0.h5', ### Change the filename for each new agent ###\n",
    "             'TRADE_FILE': 'trade_logs_fx_pair_0.bz2', ### Change the filename for each new agent ###\n",
    "             'REPLAY_FILE': 'memory_fx_pair_0.bz2', ### Change the filename for each new agent ###\n",
    "             'RF': reward_exponential_pnl, ### You can change the reward function here ###\n",
    "             'TEST_MODE': True,\n",
    "             'PRELOAD': False,\n",
    "             'UPDATE_QR': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run the RL model on the price data\n",
    "Note: To run in a local machine, please change the `TEST_MODE` to \n",
    "`False` in `rl_config`\n",
    "\"\"\"\n",
    "run(bars5m, rl_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
